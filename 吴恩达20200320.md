# Machine Learning
    -Grew out of work in AI
    -New Capability for computers
## Examples:
    -Database mining
    - Application can't program by hand
    - Self-customizing programs
    - Understanding huamn learning
## Machine learning definition
    Field of study that gives computers the ability to learn without being explicitly progeammed
    A computer programe is said to learn from experience E with respect to some task T and some performance measure P,if its performance on T, as measured by P, improves with experience E.
##  Machine learning algorithms:
    -Supervised learning
    -Unsupervised learning 计算机自己学习
    -Reinforcement learning
    -Recommender systems.
    -practical advice for learning algorithms
### Supervised learning(监督学习)
    -"right answers" given
    -Regression problem(回归问题) :
      predict continuous valued output设法预测一个连续输出
    -Classification problem(分类问题):
      Discrete valued output(0 or 1)设法预测一个离散输出
### Unsupervised Learning
    -clusting algorithm
    -
## Model representation
### 单变量线性回归
    1.training set-------->Learning Algorithm-------->h(hypathesis)
    2.size of house(x)--------->h-------->estimated price
    h maps from x's to y's
    
    Question:How do we represent h?
        y=kx+b
    以上为单变量线性回归
### cost function(代价函数)
    idea:Choose θ0，θ1 so that hθ(x) is close to y for our training examples(x,y)
    详见纸质笔记   
    平方误差代价函数
    找到J(θ)最小的地方 J(θ)是一个碗状函数
### Gradient descent(梯度下降)
    have some functionJ(θ0,θ1)
    want minimize J(θ0,θ1)
    outline:
        .start with some θ0,θ1
        .Keep changing θ0,θ1 to reduce J(θ0,θ1)
            until we hopefully end up at a minimum
    关于算法相关，详见笔记
    if α is too small, gradient descent can be slow
    if α is too large, geadient descent can overshoot the minimum. It may fail to convergem or even diverge.
#### Gradient descent for linear regression
    update θ0,θ1 simultaneously
    "Batch" gradient descent
### Feature Scaling 
    idea:Make sure features are on a similar scale
    e.g. x1=size(0-2000 feets)
         x2=number of bedrooms(1-5)
### Mean normalization
    Replace xi with xi- μi to make features have approximately zero mean(Do not apply to x0=1)
### 梯度下降学习率 
    How to make sure gradient descent is working correctly?
    How to choose learning rate α?
## Features and polynomial regression
### Normal equation
    For some linear regression problems, Normal equation can give us a much better way to solve for the optimal value of the parameters theda.
    m examples((x1,y1),....,(xm,ym)); n features
    
                comparision of two things
    Gradient Descent        |     Normal Equation
    Need to choose α           No need to choose α
    Needs  many iterations     Don't need to iterate
    Works well even when n     Need to compute (XTX)-1 
     is large                  Slow if n is very large
   
    Normal Equation如果矩阵不可逆的时候该怎么办:
        What if XTX is non-invertible?
            Redundant features(linearly dependent)
            E.g x1=size in feets²
                x2=size in m²
            Too many features(e.g. m<=n)
                Delete some features, or use regularization
    
    
    
    
